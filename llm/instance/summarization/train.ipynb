{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Finetune T5 on the California state bill subset of the BillSum dataset for abstractive summarization\n",
    "data_file = {\n",
    "    \"train\":\"your download path/data/train-00000-of-00001.parquet\",\n",
    "    \"validation\":\"your download path/data/ca_test-00000-of-00001.parquet\",\n",
    "    \"test\":\"your download path/data/test-00000-of-00001.parquet\"\n",
    "}\n",
    "model_tokenizer_path = \"your model patg\"\n",
    "dataset = load_dataset(\"parquet\", data_files=data_file)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[\"train\"][0]\n",
    "sample[\"text\"], sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_tokenizer_path)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(example):\n",
    "    texts = example[\"text\"]\n",
    "    summaries = example[\"summary\"]\n",
    "    \n",
    "    # 因为这里的texts和summaries的长度差别有点大， 所以没有放到一起统一处理，而不像在翻译任务中的数据\n",
    "    tokenized_text = tokenizer(texts, max_length=1024, truncation=True)\n",
    "    tokenized_summaries = tokenizer(text_target=summaries, max_length=128, truncation=True)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_text[\"input_ids\"],\n",
    "        \"attention_mask\":tokenized_text[\"attention_mask\"],\n",
    "        \"labels\":tokenized_summaries[\"input_ids\"]\n",
    "    }\n",
    "    # tokenized_text[\"labels\"] = tokenized_summaries[\"input_ids\"]\n",
    "    # return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(process_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解释关于tokenizer中传入一个文本还是两个文本的差别**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 明白tokenizer中传入text_target的作用：\n",
    "# 传入两个后面的会放在labels中，只传入tokenizer(text_target=example)的效果和tokenizer(sample)一样\n",
    "\n",
    "# 但是直接将sample_a和sample_b放入到tokenizer中有一个问题就是他会对这两个文本进行同样的处理，比如说阶段填充等，不灵活\n",
    "\n",
    "sample_a = \"I am very happy to learn knowledge about LLM\"\n",
    "sample_b = \"like to learn LLM\"\n",
    "tokenized_a = tokenizer(sample_a)\n",
    "tokenized_b = tokenizer(sample_b)\n",
    "tokenized_b_tar = tokenizer(text_target=sample_b)\n",
    "tokenized_a_b = tokenizer(sample_a, text_target=sample_b)\n",
    "tokenized_a, tokenized_b, tokenized_b_tar, tokenized_a_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_tokenizer_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"../../evaluate/rouge.py\")\n",
    "rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "\n",
    "    # 这里为什么是对prediction进行转化而不进行处理呢，因为prediction中是模型输出的不包含-100\n",
    "    predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # 将-100填充成为pad_token_id,where(condition, True_element, False_element)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=predictions, references=labels, use_stemmer=True)\n",
    "    print(predictions)\n",
    "    prediction_len = [np.count_nonzero(prediction) for prediction in predictions]\n",
    "\n",
    "    result[\"gen_len\"] = np.mean(prediction_len)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./checkpoint\",\n",
    "    per_device_train_batch_size=2,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=30,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"].shuffle().select(range(300)),\n",
    "    eval_dataset=tokenized_dataset[\"validation\"].shuffle().select(range(50)),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(tokenized_dataset[\"test\"].shuffle().select(range(100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"summarization\", model=model_tokenizer_path)\n",
    "\n",
    "pipe(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手动进行推断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "tokenized_text = {k:v.to(device) for k, v in tokenized_text.items()}\n",
    "generate_ids = model.generate(**tokenized_text,  max_new_tokens=100, do_sample=False)\n",
    "generate_text = tokenizer.decode(generate_ids[0])\n",
    "generate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
