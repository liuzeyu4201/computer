{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进行数据处理 => 就是转化成为model可以直接使用的tensor\n",
    "\n",
    "1. 加载tokenizer\n",
    "2. process_function函数（没有进行填充，可以使用transformers内置的处理器）\n",
    "3. DataDict进行映射，会有一个默认的参数可以拿到Dataset中的一条记录（注意：DataDict可以直接进行映射，先不要拆分成train和test）\n",
    "4. 根据任务的不同对数据设置不同的数据处理器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\software\\conda\\envs\\learn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(r\"dataset path\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_24412\\453938796.py:2: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  tokenizer = AutoTokenizer.from_pretrained(\"D:\\Desktop\\learn\\instance\\model\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your model path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # return tokenizer(examples[\"text\"], truncation=True)\n",
    "    tokenized = tokenizer(examples[\"text\"], truncation=True)\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "    labels = examples[\"label\"]\n",
    "    return {\n",
    "        \"input_ids\":input_ids,\n",
    "        \"attention_mask\":attention_mask,\n",
    "        \"labels\":labels\n",
    "    }\n",
    "tokenized_data = data.map(preprocess_function, remove_columns=data[\"train\"].column_names)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  1045,\n",
       "  2293,\n",
       "  16596,\n",
       "  1011,\n",
       "  10882,\n",
       "  1998,\n",
       "  2572,\n",
       "  5627,\n",
       "  2000,\n",
       "  2404,\n",
       "  2039,\n",
       "  2007,\n",
       "  1037,\n",
       "  2843,\n",
       "  1012,\n",
       "  16596,\n",
       "  1011,\n",
       "  10882,\n",
       "  5691,\n",
       "  1013,\n",
       "  2694,\n",
       "  2024,\n",
       "  2788,\n",
       "  2104,\n",
       "  11263,\n",
       "  25848,\n",
       "  1010,\n",
       "  2104,\n",
       "  1011,\n",
       "  12315,\n",
       "  1998,\n",
       "  28947,\n",
       "  1012,\n",
       "  1045,\n",
       "  2699,\n",
       "  2000,\n",
       "  2066,\n",
       "  2023,\n",
       "  1010,\n",
       "  1045,\n",
       "  2428,\n",
       "  2106,\n",
       "  1010,\n",
       "  2021,\n",
       "  2009,\n",
       "  2003,\n",
       "  2000,\n",
       "  2204,\n",
       "  2694,\n",
       "  16596,\n",
       "  1011,\n",
       "  10882,\n",
       "  2004,\n",
       "  17690,\n",
       "  1019,\n",
       "  2003,\n",
       "  2000,\n",
       "  2732,\n",
       "  10313,\n",
       "  1006,\n",
       "  1996,\n",
       "  2434,\n",
       "  1007,\n",
       "  1012,\n",
       "  10021,\n",
       "  4013,\n",
       "  3367,\n",
       "  20086,\n",
       "  2015,\n",
       "  1010,\n",
       "  10036,\n",
       "  19747,\n",
       "  4520,\n",
       "  1010,\n",
       "  25931,\n",
       "  3064,\n",
       "  22580,\n",
       "  1010,\n",
       "  1039,\n",
       "  2290,\n",
       "  2008,\n",
       "  2987,\n",
       "  1005,\n",
       "  1056,\n",
       "  2674,\n",
       "  1996,\n",
       "  4281,\n",
       "  1010,\n",
       "  1998,\n",
       "  16267,\n",
       "  2028,\n",
       "  1011,\n",
       "  8789,\n",
       "  3494,\n",
       "  3685,\n",
       "  2022,\n",
       "  9462,\n",
       "  2007,\n",
       "  1037,\n",
       "  1005,\n",
       "  16596,\n",
       "  1011,\n",
       "  10882,\n",
       "  1005,\n",
       "  4292,\n",
       "  1012,\n",
       "  1006,\n",
       "  1045,\n",
       "  1005,\n",
       "  1049,\n",
       "  2469,\n",
       "  2045,\n",
       "  2024,\n",
       "  2216,\n",
       "  1997,\n",
       "  2017,\n",
       "  2041,\n",
       "  2045,\n",
       "  2040,\n",
       "  2228,\n",
       "  17690,\n",
       "  1019,\n",
       "  2003,\n",
       "  2204,\n",
       "  16596,\n",
       "  1011,\n",
       "  10882,\n",
       "  2694,\n",
       "  1012,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2025,\n",
       "  1012,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  18856,\n",
       "  17322,\n",
       "  2094,\n",
       "  1998,\n",
       "  4895,\n",
       "  7076,\n",
       "  8197,\n",
       "  4892,\n",
       "  1012,\n",
       "  1007,\n",
       "  2096,\n",
       "  2149,\n",
       "  7193,\n",
       "  2453,\n",
       "  2066,\n",
       "  7603,\n",
       "  1998,\n",
       "  2839,\n",
       "  2458,\n",
       "  1010,\n",
       "  16596,\n",
       "  1011,\n",
       "  10882,\n",
       "  2003,\n",
       "  1037,\n",
       "  6907,\n",
       "  2008,\n",
       "  2515,\n",
       "  2025,\n",
       "  2202,\n",
       "  2993,\n",
       "  5667,\n",
       "  1006,\n",
       "  12935,\n",
       "  1012,\n",
       "  2732,\n",
       "  10313,\n",
       "  1007,\n",
       "  1012,\n",
       "  2009,\n",
       "  2089,\n",
       "  7438,\n",
       "  2590,\n",
       "  3314,\n",
       "  1010,\n",
       "  2664,\n",
       "  2025,\n",
       "  2004,\n",
       "  1037,\n",
       "  3809,\n",
       "  4695,\n",
       "  1012,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  2428,\n",
       "  3697,\n",
       "  2000,\n",
       "  2729,\n",
       "  2055,\n",
       "  1996,\n",
       "  3494,\n",
       "  2182,\n",
       "  2004,\n",
       "  2027,\n",
       "  2024,\n",
       "  2025,\n",
       "  3432,\n",
       "  13219,\n",
       "  1010,\n",
       "  2074,\n",
       "  4394,\n",
       "  1037,\n",
       "  12125,\n",
       "  1997,\n",
       "  2166,\n",
       "  1012,\n",
       "  2037,\n",
       "  4506,\n",
       "  1998,\n",
       "  9597,\n",
       "  2024,\n",
       "  4799,\n",
       "  1998,\n",
       "  21425,\n",
       "  1010,\n",
       "  2411,\n",
       "  9145,\n",
       "  2000,\n",
       "  3422,\n",
       "  1012,\n",
       "  1996,\n",
       "  11153,\n",
       "  1997,\n",
       "  3011,\n",
       "  2113,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  29132,\n",
       "  2004,\n",
       "  2027,\n",
       "  2031,\n",
       "  2000,\n",
       "  2467,\n",
       "  2360,\n",
       "  1000,\n",
       "  4962,\n",
       "  8473,\n",
       "  4181,\n",
       "  9766,\n",
       "  1005,\n",
       "  1055,\n",
       "  3011,\n",
       "  1012,\n",
       "  1012,\n",
       "  1012,\n",
       "  1000,\n",
       "  4728,\n",
       "  2111,\n",
       "  2052,\n",
       "  2025,\n",
       "  3613,\n",
       "  3666,\n",
       "  1012,\n",
       "  8473,\n",
       "  4181,\n",
       "  9766,\n",
       "  1005,\n",
       "  1055,\n",
       "  11289,\n",
       "  2442,\n",
       "  2022,\n",
       "  3810,\n",
       "  1999,\n",
       "  2037,\n",
       "  8753,\n",
       "  2004,\n",
       "  2023,\n",
       "  10634,\n",
       "  1010,\n",
       "  10036,\n",
       "  1010,\n",
       "  9996,\n",
       "  5493,\n",
       "  1006,\n",
       "  3666,\n",
       "  2009,\n",
       "  2302,\n",
       "  4748,\n",
       "  16874,\n",
       "  7807,\n",
       "  2428,\n",
       "  7545,\n",
       "  2023,\n",
       "  2188,\n",
       "  1007,\n",
       "  19817,\n",
       "  6784,\n",
       "  4726,\n",
       "  19817,\n",
       "  19736,\n",
       "  3372,\n",
       "  1997,\n",
       "  1037,\n",
       "  2265,\n",
       "  13891,\n",
       "  2015,\n",
       "  2046,\n",
       "  2686,\n",
       "  1012,\n",
       "  27594,\n",
       "  2121,\n",
       "  1012,\n",
       "  2061,\n",
       "  1010,\n",
       "  3102,\n",
       "  2125,\n",
       "  1037,\n",
       "  2364,\n",
       "  2839,\n",
       "  1012,\n",
       "  1998,\n",
       "  2059,\n",
       "  3288,\n",
       "  2032,\n",
       "  2067,\n",
       "  2004,\n",
       "  2178,\n",
       "  3364,\n",
       "  1012,\n",
       "  15333,\n",
       "  4402,\n",
       "  2480,\n",
       "  999,\n",
       "  5759,\n",
       "  2035,\n",
       "  2058,\n",
       "  2153,\n",
       "  1012,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 写评估函数\n",
    "\n",
    "1. 根据任务选取不同的评估指标（从Task中查找）\n",
    "2. 使用evaluate设置评估器\n",
    "3. 写模型的评估函数（要返回使用评估器计算的结果）【这里有一个默认的参数可以拿到模型计算的结果以及labels】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted labels.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
       "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple example\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.5}\n",
       "\n",
       "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
       "        >>> print(results)\n",
       "        {'accuracy': 3.0}\n",
       "\n",
       "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.8778625954198473}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"../metric_accuracy.py\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    prediction, labels = eval_pred\n",
    "    predictions = np.argmax(prediction, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练\n",
    "1. 使用AutoModelFor。。。调用预先训练的模型\n",
    "2. 对于分类任务而言，需要传入label2id和id2label\n",
    "3. 定义TrainingArguments\n",
    "4. 定义Trainer并且进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at D:\\Desktop\\learn\\instance\\model and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(r\"your model path\", num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./checkpoint\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=3e-5,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    # load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"].shuffle().select(range(1200)),\n",
    "    eval_dataset=tokenized_data[\"test\"].shuffle().select(range(100)),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 05:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.673200</td>\n",
       "      <td>0.612326</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.458700</td>\n",
       "      <td>0.610188</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.536700</td>\n",
       "      <td>0.445697</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.606936</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.352600</td>\n",
       "      <td>0.506860</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.219300</td>\n",
       "      <td>0.651298</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.639959</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.604286</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.631831</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=900, training_loss=0.30383392598893905, metrics={'train_runtime': 343.1919, 'train_samples_per_second': 10.49, 'train_steps_per_second': 2.622, 'total_flos': 393013178485968.0, 'train_loss': 0.30383392598893905, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
