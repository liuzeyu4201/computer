{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对数据处理\n",
    "**注意**：要传入model中的input和label是什么样子的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集使用的是squad\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_file = {\n",
    "                \"train\":\"your path/data/plain_text/train-00000-of-00001.parquet\",\n",
    "                \"val\":\"your path/plain_text/validation-00000-of-00001.parquet\"\n",
    "             }\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files=data_file)\n",
    "train_test_dataset = dataset[\"train\"].train_test_split(0.3)\n",
    "validation_dataset = dataset[\"val\"]\n",
    "train_test_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_start也是其实字符char的位置， 起始char + len(text) 得到末尾char\n",
    "train_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**需要面临的问题**\n",
    "1. 有一些文本比较长超过了模型可以接收的最大长度\n",
    "2. 需要根据提供的answers中的answer_start和answer_text映射出答案再上下文中的索引\n",
    "3. 找出那一部分对应于上下文，哪些部分对应于问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型使用的是DistilBERT\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your model path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 只对传入的文本进行截断\n",
    "    # return_offsets_mapping => 返回ids对字符串char之间的关系，也就是说一个ids对应原文char的索引，特殊字符用(0,0)表示且不当作字符串来表示\n",
    "    inputs = tokenizer(\n",
    "            questions,\n",
    "            examples[\"context\"],\n",
    "            truncation=\"only_second\",\n",
    "            max_length=384,\n",
    "            stride=120,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "            return_overflowing_tokens=True\n",
    "        )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    answers = examples[\"answers\"]\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # batched=True\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        # 拿到来源于哪个记录的答案\n",
    "        answer = answers[sample_idx]\n",
    "        # 注意：这里标注的是char而不是ids\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        # sequence_ids会拿到这个ids是属于inputs里面传入questions还是context呢，会使用0和1来标记，特殊字符使用None标记\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        content_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        content_end = idx -1\n",
    "        # 到上面那一步会仍然面临这char和ids对不上的问题，也就是说，多个char可能会对应一个idx\n",
    "\n",
    "        # offset是词和ids的对应关系\n",
    "        if offset[content_start][0] > end_char or offset[content_end][1] < start_char:\n",
    "            # 这个判断是答案不在文章中\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # content_start, content_end => 偏移量中的索引\n",
    "            # start_char, end_char => 文章中的字符索引\n",
    "            idx = content_start\n",
    "            # 记录文章的开始索引\n",
    "            while idx < content_end and offset[idx][0] <= start_char:\n",
    "                # idx < content_end => 在答案的offset中寻找\n",
    "                # offset[idx][0] <= start_char =>看答案的offset中的首偏移量小于或等于开始的索引\n",
    "                # 应该是在答案的第二个偏移量退出\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = content_end\n",
    "            # 记录文章的结束索引\n",
    "            while idx > content_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_validation_function(examples):\n",
    "    \"\"\"\n",
    "        整体干了什么工作？\n",
    "        1. 拿到了不同片段的id【因为一个语句会被分解成为多个lst】=> example_id\n",
    "        2. 将offset中答案的部分设置成为None\n",
    "    \"\"\"\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=120,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,\n",
    "        return_overflowing_tokens=True\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]    \n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_train_test = train_test_dataset.map(process_train_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_dataset_validation = validation_dataset.map(process_validation_function, batched=True, remove_columns=validation_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"your model path\")\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkout\",\n",
    "    per_device_train_batch_size=2,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=15,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train_test[\"train\"],\n",
    "    eval_dataset=tokenized_dataset_train_test[\"test\"].select(range(100)),\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对数据集进行后处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metrics = evaluate.load(\"../../evaluate/squad\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "predictions, _, _ = trainer.predict(tokenized_dataset_validation)\n",
    "start_logits, end_logits = predictions\n",
    "tokenized_dataset_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def compute_metrics(metric,start_logits, end_logits, features, examples, n_best=20, max_answer_length=30):\n",
    "    \"\"\"\n",
    "        examples用的是带着answer中具体答案的\n",
    "        features是tokenized的数据\n",
    "    \"\"\"\n",
    "    # example_to_features存储的是id:[数据集中索引]\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "    print(example_to_features)\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "\n",
    "        # 原始数据中的一个id以及content\n",
    "        example_id = example[\"id\"]\n",
    "        content = example[\"context\"]\n",
    "\n",
    "        # 一个实例中的很多答案\n",
    "        answers = []\n",
    "\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            # feature_index 是一个列表，其中存储了由于tokenizer中传递的stride分割成的数据索引\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best-1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best : -1].tolist()\n",
    "\n",
    "            # 在process_validation_function中是根据char_id 和 offset 去映射 开始的char在offset中的索引位置\n",
    "            # 这里的处理是已知开始的char索引位置以及offset找到char_id之后去找开始和结束的内容\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    if (end_index < start_index) or ((end_index - start_index + 1) > max_answer_length):\n",
    "                        continue\n",
    "                    answer = {\n",
    "                        \"text\":content[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\":start_logit[start_index] + end_logit[end_index]\n",
    "                    }\n",
    "                    # 假设开始的索引确定的话后面的索引可能会很多，所以需要在这里添加到列表中\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # 拿到分数最高的答案\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x:x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\":example_id, \"prediction_text\":best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append(\n",
    "                {\"id\":example_id, \"prediction_text\":\"\"}\n",
    "            )\n",
    "    therotical_answers = [{\"id\":ex[\"id\"], \"answers\":ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=therotical_answers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(metrics, start_logits, end_logits, tokenized_dataset_validation, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
