{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理数据\n",
    "1. 使用datasets进行数据的加载\n",
    "2. 加载tokenizer\n",
    "3. 写process_function\n",
    "4. 对DataDict进行映射\n",
    "5. 定义数据处理器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"wnut_17\", trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your/model/name or path\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下面想要表达的意思是命名实体识别中的标记方式**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labes = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "dataset_labes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下面想要表达的意思是:**\n",
    "tokens中的一个字符串可以被拆分成为多个ids,所以需要进行特殊的处理,需要和labels对应上\n",
    "\n",
    "`注意点1`如果不加如is_split_into_words参数会把example[\"tokens\"]中的每个元素看成一个单独的句子进行切分\n",
    "\n",
    "`注意点2`FastTokenizer的word_ids使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "tokenized_input.word_ids(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**任务中数据处理的关键**：这里需要将ner_tags和tokens对应上\n",
    "\n",
    "`注意点1`:examples的形状是{\"input_ids\":[[...],[...]],\"attention_mask\":[[...],[...]]}\n",
    "`注意点2`：word_ids是有特殊标记的token偏移的，但是label是没有的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "    labels = []         # 最终的长度应该是tokenized_inputs[\"input_ids\"]的长度是一致的\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        # 这里的word_ids和label刚好就是需要对齐处理的两条数据\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []      # 用于存储和tokenized_input[i][\"input_ids\"]对齐的标签信息\n",
    "        # 接下来的工作就是根据word_ids, label转化到label_ids中然后将label_ids放到labels中\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != previous_word_idx:\n",
    "                # 如何和前面不一样就添加到label中\n",
    "                label_ids.append(label[word_id])\n",
    "            else:\n",
    "                # 和前面的一样但不是None（因为字词应该被忽略掉，只看第一个）\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_id\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = dataset[\"train\"].select(range(3))\n",
    "# train_dataset.map(process_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_dataset = dataset.map(process_function,batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备训练\n",
    "1. 创建模型（确定创建模型的时候是否要传入特殊的config）\n",
    "2. 定义评估函数\n",
    "3. 准备TrainingArguments\n",
    "4. 准备Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "id2label = {index:value for index, value in enumerate(dataset_labes)}\n",
    "label2id = {value:index for index, value in enumerate(dataset_labes)}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"your/model/name or path\", num_labels=len(dataset_labes), id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "seqeval = load(\"../../evaluate/seqeval.py\")\n",
    "def compute_metrics(eval_pred):\n",
    "    # 这里是以batch_size的形式传入的，所以需要两层解耦\n",
    "    # 而且seqeval传入的predictions和references需要的是命名实体识别的字符串类型数据，而不是数值型数据，所以需要根据数值型数据映射到字符串中\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    true_predictions = [\n",
    "        [dataset_labes[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [dataset_labes[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    print(results)\n",
    "    return {\n",
    "        \"precision\":results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }\n",
    "seqeval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoint',\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=300,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_steps=3,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"].shuffle(),\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
