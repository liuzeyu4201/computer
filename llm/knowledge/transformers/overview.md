# å¿«é€Ÿå¼€å§‹

## Pipeline

- åªä¼ å…¥ä¸€ä¸ªä»»åŠ¡ç±»å‹

```python
from transformers import pipeline
classifier = pipeline("sentiment-analysis")
# å¦‚æœä¼ å…¥çš„åªæœ‰ä¸€æ¡é‚£ä¹ˆå°±ä¼ å…¥å­—ç¬¦ä¸²ï¼Œå¦‚æœæ˜¯å¤šæ¡çš„è¯å°±éƒ½æ”¾åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­
results = classifier(["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
```

- ä¼ å…¥ç‰¹å®šçš„modelå’Œtokenizer

```python
from transformers import AutoTokenizer, AutoModel, pipeline
model_name = "your/model/path"
model = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline("sentiment-analysis", model, tokenizer)
```

## AutoClass

### AutoTokenizer

å·²ç»åšè¿‡è¯¦ç»†çš„ä»‹ç»

### AutoModel

transformersæä¾›äº†ä¸€ä¸ªç®€å•ç»Ÿä¸€çš„æ–¹æ³•æ¥å¯¹æ¨¡å‹è¿›è¡ŒåŠ è½½ï¼Œå¯ä»¥åƒä½¿ç”¨AutoTokenizerä¸€æ ·ä½¿ç”¨AutoModelï¼Œå”¯ä¸€éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºä¸åŒçš„ä»»åŠ¡é€‰æ‹©ä¸åŒçš„åŠ è½½ç±»ï¼Œæ¯”å¦‚å¯¹äºæ–‡æœ¬åˆ†ç±»ï¼Œéœ€è¦é€‰æ‹©`AutoModelForSequenceClassification`

é»˜è®¤æƒ…å†µä¸‹ä¼šä½¿ç”¨torch.float32è¿›è¡Œæ¨¡å‹çš„åŠ è½½ï¼Œå¦‚æœä¼ å…¥å‚æ•°torch_type='auto'ï¼Œé‚£ä¹ˆå°±ä¼šæŒ‰ç…§æ¨¡å‹ä¸­çš„é…ç½®æ–‡ä»¶æ¥ç¡®å®šæ¨¡å‹çš„ç²¾åº¦



### save model

```python
save_path = 'your/model/path'
tokenizer.save_pretrained(save_path)
model.save_pretrained(save_path)
```



## Custom model builds

- ä¿®æ”¹é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„ç‰¹å®šå‚æ•°

  ```python
  from transformers import AutoConfig
  
  my_config = AutoConfig.from_pretrained("your/model/path", n_heads=12)
  ```

- åŠ è½½æ¨¡å‹

  ```python
  from transformers import AutoModel
  
  my_model = AutoModel.from_config(my_config)
  ```

  

## Trainer

1. åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹

   ```python
   from transformers import AutoModel
   model = AutoModel.from_pretrained("your/model/path")
   ```

2. å®šä¹‰`TrainingArguments`ï¼ŒåŒ…å«äº†æ¨¡å‹è®­ç»ƒæ—¶å€™çš„è¶…å‚æ•°

   ```python
   from transformers import TrainingArguments
   # å¦‚æœæ²¡æœ‰ä¼ å…¥çš„è¯å°†ä¼šä½¿ç”¨é»˜è®¤å€¼
   training_args = TrainingArguments(
       output_dir="path/to/save/folder/",
       learning_rate=2e-5,
       per_device_train_batch_size=8,
       per_device_eval_batch_size=8,
       num_train_epochs=2,
   )
   ```

3. åŠ è½½å¤„ç†å™¨

   ```python
   from transformers import AutoTokenizer
   
   tokenizer = AutoTokenizer.from_pretrained("your/tokenizer/path")
   ```

4. åŠ è½½æ•°æ®é›†

   ```python
   from datasets import load_dataset
   dataset = load_dataset("dataset/name")
   ```

5. å®šä¹‰å¤„ç†å‡½æ•°å¹¶ä¸”å°†æ•°æ®é›†ä¸­çš„å…ƒç´ è½¬åŒ–æˆä¸ºæ•°å€¼æ•°æ®

   ```python
   def tokenize_dataset(example):
       # è¿™é‡Œè¿”å›çš„æ˜¯æ•°æ®é›†ä¸­ä¸€æ¡å…ƒç´ è¦è¢«tokenizeçš„å…ƒç´ 
       return ....
   dataset = dataset.map(tokenize_dataset, batched=True)
   ```

6. å¯¹æ•°æ®è¿›è¡Œå¡«å……ï¼Œå¡«å……åæ¨¡å‹å¯ä»¥ä½¿ç”¨

   ```python
   from transformers import DataCollatorWithPadding
   data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   ```

7. å®šä¹‰Trainerå¹¶ä¸”å¼€å§‹è®­ç»ƒ

   ```python
   from transformers import Trainer
   
   trainer = Trainer(
       model=model,
       args=training_args,
       train_dataset=dataset["train"],
       eval_dataset=dataset["test"],
       processing_class=tokenizer,
       data_collator=data_collator,
   )
   # å¼€å§‹å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒ
   trainer.train()
   ```





# æ•™ç¨‹

## ä½¿ç”¨Piplinesè¿›è¡Œæ¨å¯¼

