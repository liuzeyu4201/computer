# å¿«é€Ÿå¼€å§‹

## Pipeline
- pipeline æ˜¯ Hugging Face æä¾›çš„ä»»åŠ¡çº§æ¥å£ï¼Œè®©ä½ æ— éœ€å…³å¿ƒæ¨¡å‹ç»†èŠ‚å³å¯å®Œæˆè¯¸å¦‚æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€ç¿»è¯‘ã€æ‘˜è¦ã€ç”Ÿæˆç­‰ä»»åŠ¡ã€‚
- pipline å…è®¸å¤šç§ä»»åŠ¡ç±»å‹
![piplineFeatures](./overview.assets/pipline.png)
- åªä¼ å…¥ä¸€ä¸ªä»»åŠ¡ç±»å‹

```python
from transformers import pipeline
classifier = pipeline("sentiment-analysis")
# å¦‚æœä¼ å…¥çš„åªæœ‰ä¸€æ¡é‚£ä¹ˆå°±ä¼ å…¥å­—ç¬¦ä¸²ï¼Œå¦‚æœæ˜¯å¤šæ¡çš„è¯å°±éƒ½æ”¾åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­
results = classifier(["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
```

- ä¼ å…¥ç‰¹å®šçš„modelå’Œtokenizer

```python
from transformers import AutoTokenizer, AutoModel, pipeline
model_name = "your/model/path"
model = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline("sentiment-analysis", model, tokenizer)
# æ³¨æ„ modelå’Œ tokenizer è½½å…¥çš„model_name éœ€è¦åŒ¹é…
```

## AutoClass

### AutoTokenizer

å·²ç»åšè¿‡è¯¦ç»†çš„ä»‹ç»

### AutoModel

transformersæä¾›äº†ä¸€ä¸ªç®€å•ç»Ÿä¸€çš„æ–¹æ³•æ¥å¯¹æ¨¡å‹è¿›è¡ŒåŠ è½½ï¼Œå¯ä»¥åƒä½¿ç”¨AutoTokenizerä¸€æ ·ä½¿ç”¨AutoModelï¼Œå”¯ä¸€éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºä¸åŒçš„ä»»åŠ¡é€‰æ‹©ä¸åŒçš„åŠ è½½ç±»ï¼Œæ¯”å¦‚å¯¹äºæ–‡æœ¬åˆ†ç±»ï¼Œéœ€è¦é€‰æ‹©`AutoModelForSequenceClassification`

é»˜è®¤æƒ…å†µä¸‹ä¼šä½¿ç”¨torch.float32è¿›è¡Œæ¨¡å‹çš„åŠ è½½ï¼Œå¦‚æœä¼ å…¥å‚æ•°torch_type='auto'ï¼Œé‚£ä¹ˆå°±ä¼šæŒ‰ç…§æ¨¡å‹ä¸­çš„é…ç½®æ–‡ä»¶æ¥ç¡®å®šæ¨¡å‹çš„ç²¾åº¦



### save model

```python
save_path = 'your/model/path'
tokenizer.save_pretrained(save_path)
model.save_pretrained(save_path)
```



## Custom model builds

- ä¿®æ”¹é¢„è®­ç»ƒæ¨¡å‹ä¸­çš„ç‰¹å®šå‚æ•°

  ```python
  from transformers import AutoConfig
  
  my_config = AutoConfig.from_pretrained("your/model/path", n_heads=12)
  ```

- åŠ è½½æ¨¡å‹

  ```python
  from transformers import AutoModel
  
  my_model = AutoModel.from_config(my_config)
  ```

  

## Trainer

1. åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹

   ```python
   from transformers import AutoModel
   model = AutoModel.from_pretrained("your/model/path")
   ```

2. å®šä¹‰`TrainingArguments`ï¼ŒåŒ…å«äº†æ¨¡å‹è®­ç»ƒæ—¶å€™çš„è¶…å‚æ•°

   ```python
   from transformers import TrainingArguments
   # å¦‚æœæ²¡æœ‰ä¼ å…¥çš„è¯å°†ä¼šä½¿ç”¨é»˜è®¤å€¼
   training_args = TrainingArguments(
       output_dir="path/to/save/folder/",
       learning_rate=2e-5,
       per_device_train_batch_size=8,
       per_device_eval_batch_size=8,
       num_train_epochs=2,
   )
   ```

3. åŠ è½½ç¼–ç å™¨

   ```python
   from transformers import AutoTokenizer
   
   tokenizer = AutoTokenizer.from_pretrained("your/tokenizer/path")
   ```

4. åŠ è½½æ•°æ®é›†

   ```python
   from datasets import load_dataset
   dataset = load_dataset("dataset/name")
   ```

5. å®šä¹‰å¤„ç†å‡½æ•°å¹¶ä¸”å°†æ•°æ®é›†ä¸­çš„å…ƒç´ è½¬åŒ–æˆä¸ºæ•°å€¼æ•°æ®

   ```python
   def tokenize_dataset(example):
       # è¿™é‡Œè¿”å›çš„æ˜¯æ•°æ®é›†ä¸­ä¸€æ¡å…ƒç´ è¦è¢«tokenizeçš„å…ƒç´ 
       return ....
   dataset = dataset.map(tokenize_dataset, batched=True)
   ```

6. å¯¹æ•°æ®è¿›è¡Œå¡«å……ï¼Œå¡«å……åæ¨¡å‹å¯ä»¥ä½¿ç”¨

   ```python
   from transformers import DataCollatorWithPadding
   data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   ```

7. å®šä¹‰Trainerå¹¶ä¸”å¼€å§‹è®­ç»ƒ

   ```python
   from transformers import Trainer
   
   trainer = Trainer(
       model=model,
       args=training_args,
       train_dataset=dataset["train"],
       eval_dataset=dataset["test"],
       processing_class=tokenizer,
       data_collator=data_collator,
   )
   # å¼€å§‹å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒ
   trainer.train()
   ```

# æ•™ç¨‹

## ä½¿ç”¨Piplinesè¿›è¡Œæ¨å¯¼

### é€šç”¨

```
pipelineå¯ä»¥ä¼ å…¥çš„å‚æ•°
- task			# ä»»åŠ¡ç±»å‹
- model			# ç‰¹å®šçš„modelæ˜¯ä¸€ä¸ªpath
- tokenizer		# ç‰¹å®šçš„tokenizeræ˜¯ä¸€ä¸ªpath
- torch_dtype	# åŠ è½½æ¨¡å‹çš„æƒé‡ç±»å‹ï¼Œè®¾ç½®autoä¼šæ ¹æ®é…ç½®æ–‡ä»¶è¿›è¡ŒåŠ è½½
- device		# GPUç¼–å·
- batch_size	# æ‰¹é‡å¤§å°
- other			# ä¸åŒçš„ä»»åŠ¡ç±»å‹å¯ä»¥ä¼ å…¥çš„é¢å¤–å‚æ•°ä¸åŒ
```

### å¦‚ä½•ç¡®å®šå…¶ä»–å¯ä»¥ä¼ å…¥çš„å‚æ•°ï¼Ÿ

è¯•è¯•pipline.config



### ä½¿ç”¨accelerate 

```python
pipe = pipeline(model="facebook/opt-1.3b", device_map="auto", model_kwargs={"load_in_8bit": True})
# device_map="auto" => è‡ªåŠ¨æ˜ å°„åˆ°å¯ä»¥ä½¿ç”¨çš„è®¾å¤‡ä¸Š
# "load_in_8bit":True => ä¸‹è½½bitsandbyteså¯ä»¥è¿›è¡Œé‡åŒ–
```

**æ³¨**ï¼šdevice_map = "auto"é€šå¸¸æ˜¯åœ¨æ¨æ–­çš„æ—¶å€™ä½¿ç”¨



## å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹

### ä½¿ç”¨Trainer

**æ³¨**ï¼šä¸Šé¢Trainerçš„ç« èŠ‚ç®€å•è¯´æ˜äº†æ€ä¹ˆå¯¹ä¸€ä¸ªæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä½†æ˜¯å¯¹äºå®Œæ•´çš„è®­ç»ƒæ¨¡å‹è¿˜ç¼ºå°‘è¯„ä¼°çš„éƒ¨åˆ†ï¼Œè¿™ä¸ªéƒ¨åˆ†å°†å¯¹æ­¤è¿›è¡Œè¡¥å……

- é¦–å…ˆéœ€è¦å¯¹äºä¸åŒçš„ä»»åŠ¡é€‰å®šä¸åŒçš„æŒ‡æ ‡

- ç¡®å®šè¯„ä¼°å‡½æ•°

  ```python
  def compute_metrics(eval_pred):
      logits, labels = eval_pred
      predictions = np.argmax(logits, axis=-1)
      return metric.compute(predictions=predictions, references=labels)
  ```

  

- é…ç½®TrainingArgumentsä¸­è¯„ä¼°çš„ç›¸å…³å‚æ•°

- å°†è¯„ä¼°å‡½æ•°ä½œä¸ºå‚æ•°ä¼ å…¥Trainer

  ```python
  trainer = Trainer(
      model=model,
      args=training_args,
      train_dataset=small_train_dataset,
      eval_dataset=small_eval_dataset,
      # è¯„ä¼°å‡½æ•°
      compute_metrics=compute_metrics,
  )
  ```

  



### ä½¿ç”¨åŸç”Ÿpytorch

1. å¯¹äºæ•°æ®çš„å¤„ç†

   - è‹¥æ²¡æœ‰ä½¿ç”¨datasetsåº“ï¼Œä¹Ÿå°±æ˜¯æ²¡æœ‰è¿”å›Datasetå¯¹è±¡ï¼Œé‚£ä¹ˆéœ€è¦æ‰‹å†™Dataset
   - å¯ä»¥ç›´æ¥å’Œdatasetsåº“ä¸­çš„Datasetå¯¹è±¡è¿›è¡Œé€‚é…

2. åˆ›å»ºDataLoader

   ```python
   from torch.utils.data import DataLoader
   
   train_dataloader = DataLoader(train_data, shuffle=True, batch_size=8)
   test_dataloader = DataLoader(test_data, shuffle=False, batch_size=8)
   # å¦‚æœtrain_dataè¿˜æ²¡æœ‰ç»è¿‡tokenizeé‚£ä¹ˆéœ€è¦ä¼ å…¥å¤„ç†çš„å‡½æ•°
   ```

3. å‡†å¤‡æ¨¡å‹ï¼Œä¼˜åŒ–å™¨ï¼Œè°ƒåº¦å™¨

   ```python
   from torch.optim import AdamW
   from transformers import AutoModelFor...
   from transformers import get_scheduler
   
   model = AutoMolFor....from_pretrained('your/model/path')
   optimizer = AdamW(model.parameters(), lr=5e-5)
   num_epochs = 3
   num_training_steps = num_epochs * len(train_dataloader)
   lr_scheduler = get_scheduler(
       name="linear",
       optimizer=optimizer,
       num_warmup_steps=300,
       num_training_steps=num_training_steps
   )
   # å°†æ¨¡å‹ç§»åŠ¨åˆ°device
   model.to(device)
   ```

4. è®­ç»ƒä»£ç 

   ```python
   from tqdm.auto import tqdm
   # è¿›åº¦æ¡
   process_bar = tqdm(range(num_training_steos))
   model.train()
   for epoch in range(num_epochs):
       for batch in train_dataloader:
           batch = {k:v.to(device) for k, v in batch.items()}
           outputs = model(**batch)
           loss = outputs.loss
           loss.backward()
           optimizer.stemp()
           lr_scheduler.step()
           optimizer.zero_grad()
           process_bar.update(1)
   ```

5. è¯„ä¼°ä»£ç 

   ```python
   import evaluate
   
   metric = evaluate.load("accuracy")
   
   model.eval()
   for batch in test_dataloader:
       batch = {k:v.to(device) for k, v in batch.items()}
       with torch.no_grad():
           output = model(**batch)
       logits = output.logits
       predictions = torch.argmax(logits, axis=-1)
       metric.add_batch(predictions=predictions, references=batch["labels"])
   
   metric.compute()
   ```





### ä½¿ç”¨accelerateè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ

- ç¬¬ä¸€æ­¥åˆ›å»ºAccelerateå¯¹è±¡
- å¯¹æ¨¡å‹ã€ä¼˜åŒ–å™¨ï¼ŒDataLoaderè¿›è¡ŒåŒ…è£…
- åå‘ä¼ æ’­çš„æ—¶å¯¹lossè¿›è¡Œç‰¹æ®Šå¤„ç†
- ä½¿ç”¨accelerateæŒ‡ä»¤è¿›è¡Œå¯åŠ¨

```python
from accelerate import Accelerator

# ...å¯¼åŒ…

# 1.åˆå§‹åŒ–Accelerate
accelerator = Accelerator()

# æ¨¡å‹ä¸è¦å¾€ç‰¹å®šçš„è®¾å¤‡ä¸Šæ”¾
model = AutoModelFor....from_pretrained('your/model/path')
optimzer = AdamW(model.parameters(),lr=3e-5)

model, optimizer, train_dataloader, test_dataloader = accelerator(model, optimizer, train_dataloader, test_dataloader)

### ä¸‹é¢çš„ä»£ç å’Œä¸Šé¢ä½¿ç”¨åŸç”Ÿpytorchè®­ç»ƒçš„ä»£ç æœ‰å¾®å°å·®åˆ«
from tqdm.auto import tqdm
# è¿›åº¦æ¡
process_bar = tqdm(range(num_training_steos))
model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        # ä¸è¦å¾€deviceä¸Šæ”¾
        # batch = {k:v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        # å¦‚æœä½¿ç”¨accelerateè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒçš„è¯outputs.losséœ€è¦ä¸€ä¸ªæ•°å€¼è¿›è¡Œæ¥æ”¶
        loss = outputs.loss
        # ä¸è¦ä½¿ç”¨ä¸‹é¢çš„æ–¹å¼è¿›è¡Œåå‘ä¼ æ’­
        # loss.backward()
        accelerator.backward(loss)
        optimizer.stemp()
        lr_scheduler.step()
        optimizer.zero_grad()
        process_bar.update(1)
```

**æ³¨**ï¼šå¦‚æœä½¿ç”¨Trainerè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒåªéœ€è¦åœ¨TrainingArugmentè®¾å®šddp_find_unused_parameterså³å¯



## ä½¿ç”¨PEFTåŠ è½½å’Œè®­ç»ƒé€‚é…å™¨

å…·ä½“peftæ”¯æŒçš„é«˜æ•ˆå¾®è°ƒæ–¹æ³•è¯·åœ¨peftç¬”è®°ä¸­å¯»æ‰¾

1. å‡†å¤‡æ¨¡å‹

   ```python
   from transformers import AutoModelForCasualLM, AutoTokenizer, BitsAndBytesConfig
   
   model_id = "your/model/name or path"
   model = AutoModelForCasualLM.from_pretrained(model_id)
   # ä½¿ç”¨é‡åŒ–åŠ è½½
   model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))
   
   ```

2. åŠ è½½adapter

   ```python
   peft_model_id = "your/adapter/name or path"
   model.load_adapter(peft_model_id)
   ```

3. åƒæ¨¡å‹ä¸­æ·»åŠ adapter

   ```python
   from peft import LoraConfig
   lora_config = LoraConfig(
       target_modules=["q_proj", "k_proj"],
       init_lora_weights=False
   )
   
   # æ·»åŠ ä¸€ä¸ªåå­—æ˜¯adapter_1çš„lora adapter
   model.add_adapter(lora_config, name="adapter_1")
   # æ·»åŠ ä¸€ä¸ªåå­—æ˜¯adapter_2çš„lora adapter
   model.add_adapter(lora_config, name="adapter_2")
   ```

4. é€‰æ‹©è¦æ¿€æ´»çš„adapter(ä¸€æ¬¡åªèƒ½æœ‰ä¸€ä¸ªadapterå¤„äºæ¿€æ´»çŠ¶æ€)

   ```python
   # è¿™é‡Œé€‰æ‹©name = "adapter_1"åŒæ ·ä¹Ÿå¯ä»¥é€‰æ‹©name="adapter_2"
   model.set_adapter("adapter_1")
   model.generate(**input)
   print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))
   ```

5. æ¿€æ´»å’Œç¦ç”¨adapter

   ```python
   # è¿™é‡Œçš„modelæŒ‡çš„æ˜¯peftmodel
   model.enable_adapters()
   # ç¦ç”¨
   model.disable_adapters()
   ```

6. è®­ç»ƒ

   ```python
   # modelæŒ‡çš„æ˜¯PeftModel
   trainer = Trainer(model=model,...)
   trainer.train()
   ```

7. ä¿å­˜å’ŒåŠ è½½

   - åŠ è½½æœ‰è¿™adapterçš„model

   ```python
   # è¿™é‡Œçš„modelæ˜¯å·²ç»ä½¿ç”¨Trainerè®­ç»ƒè¿‡çš„
   model.save_pretrained(save_dir)
   
   model = AutoModelForCausalLM.from_pretrained(save_dir)
   ```

   - åŠ è½½æ²¡æœ‰adapterçš„model

   ```python
   from peft import PeftConfig, PeftModel
   base_model = AutoModelForCasualLM.from_pretrained(model_id)
   lora_adapter_id = "your/adapter/name or path"
   
   lora_config = PeftConfig.from_pretrained(lora_adapter_id)
   
   peft_model = PeftModel.from_pretraied(base_model, lora_config)
   ```

8. å¯ä»¥å°†åŸå§‹æ¨¡å‹çš„å…¶ä»–å±‚å‚ä¸åˆ°æ¨¡å‹çš„è®­ç»ƒå¹¶ä¸”ä¿å­˜æƒé‡

   ```python
   lora_config = LoraConfig(
       target_modules=["q_proj", "k_proj"],
       # éœ€è¦ä¼ å…¥çš„å‚æ•°
       modules_to_save=["lm_head"],
   )
   ```

   

## agent

## generateå‡½æ•°

### åŸºæœ¬ä½¿ç”¨

1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨

   ```python
   from transformers import AutoModelForCasualLM, AutoTokenizer
   
   tokenizer = AutoTokenizer.from_pretrained("tokenizer_id")
   model = AutoModelForCasualML.from_pretrained("model_id")
   ```

2. ç”Ÿæˆæ–‡æœ¬

   ```python
   model_inputs = tokenizer("what you want to ask", return_tensors="pt").to(device)
   
   # é»˜è®¤æƒ…å†µä¸‹çš„æœ€å¤§é•¿åº¦æ˜¯20
   generate_ids = model.generate(**model_inputs)
   response = tokenizer.decode(generate_ids,skip_special_tokens=True)[0]
   ```



### å¸¸è§å‘

- æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦

  ```python
  model.generate(**model_inputs, max_new_tokens=nums)
  ```

- ä¸æ­£ç¡®çš„ç”Ÿæˆæ–¹å¼(å¯¹äºåŒä¸€ä¸ªé—®é¢˜æ¯æ¬¡è¾“å‡ºçš„ç­”æ¡ˆæ˜¯ä¸€è‡´çš„ï¼Œæ²¡æœ‰åˆ›é€ æ€§)

  ```python
  model.generate(**model_inputs, do_sample=True)
  ```

- é”™è¯¯çš„å¡«å……ä¾§

  ```python
  # å¯¹äºdecode-onlyæ¶æ„çš„æ¨¡å‹è®­ç»ƒçš„æ—¶å€™padè¦é€‰æ‹©rightï¼Œæ¨æ–­çš„æ—¶å€™padéœ€è¦é€‰æ‹©leftï¼Œgenerateå‡½æ•°ç”¨äºæ¨æ–­
  tokenizer = AutoTokenizer.from_pretrained("tokenizer_id", return_tensors="pt",padding_side="left")
  tokenizer.pad_token = tokenizer.eos_token		# å¾ˆå¤šçš„LLMæ²¡æœ‰ç‰¹æ®Šçš„å¡«å……token
  ```

- é”™è¯¯çš„prompt

  - å¯¹äºä¸€äº›ç‰¹æ®Šçš„æ¨¡å‹å¦‚æœä¸ä½¿ç”¨å®ƒæŒ‡å®šçš„æ¨¡æ¿ï¼Œé‚£ä¹ˆæ¨¡å‹çš„æ€§èƒ½å°±ä¼šä¸‹é™

    ```python
    messages = [
        {
            "role": "system",
            "content": "You are a friendly chatbot who always responds in the style of a thug",
        },
        {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
    ]
    model_inputs = tokenizer.apply_chat_template(message, add_generation_prompt=True,max_new_tokens=20)
    generated_ids = model(**model_inputs)
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
    ```



## å’Œtransformersè¿›è¡ŒèŠå¤©

### ç®€å•ä½¿ç”¨

å¦‚æœä½ å¯ä»¥ç»™æ¨¡å‹ä¼ å…¥ä¸€ä¸ªå†å²çš„èŠå¤©è®°å½•ï¼Œé‚£ä¹ˆæ¨¡å‹å¯ä»¥å¯ä»¥åŸºäºä½ ä¼ å…¥çš„å†å²æ¥ç€ç»™ä½ å›å¤

1. å†å²è®°å½•

   ```python
   chat = [
       {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
       {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
   ]
   ```

2. ä½¿ç”¨piplineåŠ è½½æ¨¡å‹

   ```python
   import torch
   from transformers import pipline
   
   # ä½¿ç”¨åŠç²¾åº¦åŠ è½½æ¨¡å‹
   pipe = pipline(task=text-generation,model="model_id",torch_dtype=torch.bfloat16, device_map="auto")
   
   response = pipe(chat, max_new_tokens=512)
   chat = response[0]['generated_text']
   chat.append(
       {"role": "user", "content": "Wait, what's so wild about soup cans?"}
   )
   response = pipe(chat, max_new_tokens=512)
   ```



### å¦‚ä½•é€‰æ‹©æ¨¡å‹

- æ¨¡å‹å°ºå¯¸
- æŸ¥çœ‹æ¨¡å‹æ’è¡Œæ¦œ
- ç‰¹å®šçš„é¢†åŸŸï¼ˆä»»åŠ¡ï¼‰



### pipelineå†…éƒ¨æ€ä¹ˆè¿è¡Œçš„

**æ³¨**ï¼šå¯¹æ¯”ç®€å•ä½¿ç”¨ä¸­ä½¿ç”¨pipelineè¿›è¡Œæ¨æ–­

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

chat = [
    {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
]

# 1: åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto", torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")

# 2: åº”ç”¨chatæ¨¡æ¿
formatted_chat = tokeinzer.apply_chat_template(chat, tokenize=False, add_generation=True)

# 3: è½¬åŒ–ä¸ºæ¨¡å‹å¯ä»¥ä½¿ç”¨çš„æ•°æ®
inputs = tokenizer(formatted_chat, add_special_tokens=False, return_tensors="pt")
inputs = {k:v.to(device) for k, v in inputs.items()}

# 4: ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­
outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.1)

# 5: è§£ç 
decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)

```





## è¡¥å……

æ¨¡å‹ç±»å‹å’Œæ¨¡å‹æ¶æ„ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼ˆGPTç»™å‡ºï¼‰

![image-20250307194630121](../transformers/overview.assets/image-20250307194630121.png)









